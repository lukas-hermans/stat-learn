\subsection{K-Means Clustering}

For the application of K-means clustering, a suitable choice for the hyperparameter $K$ - the desired number of clusters - has to be found. In order to do so, Fig.~\ref{fig:G_vs_K} reports the total within-cluster variation $G$ as a function of the number of clusters $K$. A decrease of the total within-cluster variation $G$ means that the words in each cluster become more similar. However, the within-cluster variation $G$ is a strictly decreasing function of $K$. Thus, a smaller $G$ does not necessarily mean that the clustering is more accurate as a larger $K$ leads \textit{always} to a smaller $G$. A reasonable tradeoff is to chose $K$ such that the decrease of $G$ as a function of the cluster number $K$ has already become small. In other words, $K$ is taken in an interval where the total within-cluster variation decreases only slightly in comparison to its initial decrease for small values of $K$. Usually, this method is called ellbow method as the most suitable value of $K$ lies in an ellbow-shaped part of the curve. In the present case, the value $K=8$ - indicated by the dashed, red line in the Figure - marks a point in such an ellbow, and is used for further analyses.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{G_vs_K.pdf}
\caption{Total within-cluster variation $G$ as a function of the number of clusters $K$ using K-means clustering. The dashed, red line indicates the value $K=8$ which is used for the further analysis based on the ellbow method, see the main text for more details. The most common Tweeted words in the $K=8$ clusters are reported in Fig.~\ref{fig:K_cluster}, while the number of Tweets in each cluster is summarized in Tab.~\ref{tab:tweets_per_cluster}.}
\label{fig:G_vs_K}
\end{figure}

If the desired number of clusters is set to $K=8$, the Tweets are distributed among the clusters as reported in Tab.~\ref{tab:tweets_per_cluster}. Most remarkably is cluster $3$ with $9693$ associated Tweets. Note that the numbering of the clusters is arbitrary, and does not indicate any hierarchy among the clusters.

\begin{table}[h!]
\centering
\begin{tabular}{c||c|c|c|c|c|c|c|c} 
cluster & $1$ & $2$ &$3$ &$4$ &$5$ &$6$ &$7$ &$8$ \\
 \hline
\# Tweets & $154$ & $831$ &$9693$ &$128$ &$113$ &$413$ &$102$ &$283$
\end{tabular}
\caption{Number of Tweets per cluster for the application of K-means clustering with $K=8$ clusters. The total number of tweets is $11717$.}
\label{tab:tweets_per_cluster}
\end{table}

Fig.~\ref{fig:K_cluster} shows the $10$ most common words in the Tweets in each of the $K=8$ clusters. The numbering of the clusters is the same as in Tab.~\ref{tab:tweets_per_cluster}. \\

Cluster $3$ mainly contains positive emotions like \enquote{good}, \enquote{great}, and \enquote{yeah}. These are connected to the term \enquote{model}. There is also the word \enquote{coming}. A summary of cluster $3$ could be Elon Musk's euphoria towards a \enquote{coming model}. 

Cluster $2$ and cluster $8$ give insight in what is meant with the term \enquote{model}. Both clusters show a connection with the word \enquote{tesla}. Especially cluster $8$ reveals that the term \enquote{model} refers to a new \enquote{car}. Additionally, the \enquote{electric} car might have an \enquote{autopilot}. 

Cluster $7$ includes mostly energy-related terms such as \enquote{solar}, \enquote{power}, and \enquote{battery}. Here, also the term \enquote{tesla} - that is related to cars - occurs. 

The clusters $1$, $4$, and $6$ are related to rockets and space. In cluster $1$, especially the planet \enquote{mars} appears to be of interest to Elon Musk. Cluster $4$ reveals that \enquote{raptor} might be the name of a rocket \enquote{engine}. In general, the terms \enquote{falcon} and \enquote{starship} seem to be related to rockets though it is not clear what they specifically describe.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{K_cluster.pdf}
\caption{Barplot of the $10$ most common Tweeted words in $K=8$ clusters using K-means clustering. The meaning of the clusters in detail is discussed in the main text.}
\label{fig:K_cluster}
\end{figure}

\subsection{Hierarchical Clustering}

In contrast to the K-means clustering from the last Subsection, the hierarchical clustering does not require an a priori selection of the desired number of groups. However, as explained in the theory Section, a distance measure between the words as well as the linkage as a measure of dissimilarity between clusters has to be defined. In the present work, the binary distance (also called Jaccard distance) in combination with Ward's minimum variance method as the linkage are found to lead to the most interpretable clustering results. Note that Ward's minimum variance method is originally designed to be combined with the Euclidean distance. Nonetheless, given the excellent results of the combination with the binary distance, this combination underlies the following discussion of the hierarchical clustering.\\

Fig.~\ref{fig:hclust_dendogram} shows the so-called dendogram of the application of the hierarchical clustering to the $140$ words of the term-document matrix. Each leaf of the dendogram is one word. The dendogram reports how the single words are merged to clusters, and how these clusters are fused to larger clusters. The root of the dendogram is a cluster that contains all words. The vertical axis at the lefthand side of the dendogram shows the value of the dissimilarity measure (in this case Ward's minimum variance method in combination with the binary distance) at the respective height of the dendogram. Clusters that are merged at a lower height are more similar than clusters that are merged at a higher point of the dendogram. The red boxes in the Figure indicate $15$ clusters. The words in each cluster are visualized as wordclouds in Fig.~\ref{fig:hclust_wordclouds}. Note that the number of clusters does not have to be the same as for the K-means clustering because the clusters are heuristic categorizations of the words. Necessarily, this categorization depends on the applied clustering technique. In addition, here, words are clustered, whereas in the previous n Tweets were clustered. \\

The clusters $1$, $2$, $3$, $10$, and $11$ are in relation to Elon Musk's interest into rockets and space. In particular, new information in comparison to the results from the K-means clustering is obtained from cluster $3$. Here, it appears that Elon Musk announces test flights and space missions that take place \enquote{tomorrow} or \enquote{today}. Thus, his interest into space seems to be related to particular events, i.e. rocket launches.

The clusters $5$, $6$, and $8$ are related to tesla, and his interest in cars. Most importantly, cluster $8$ shows that Elon Musk also announces the release of cars (indicated by the word \enquote{drive}) in the next \enquote{days} or \enquote{weeks}.

Cluster $7$ regards announcments of \enquote{coming software updates}. From the words in the cluster, it is not clear to what software these announcments refer to.

While cluster $15$ is focused on the topic \enquote{energy}, cluster $4$ contains completely new information that was not visible in the results of the K-means clustering above. Apparently, Elon Musk is also interested in another company that is related to \enquote{tunnels}. He seems to be particularly fascinated by the \enquote{boring} process of these tunnels.

Finally, the remaining clusters $9$, $12$, $13$, and $14$ do not allow the retrieval of further information about Elon Musk. They seem to be simply accumulations of losely related words. A reason for the observation of these clusters is that the hierarchical clustering puts \textit{all} words into a cluster. However, there might be words and Tweets that have a single meaning, and are not really part of a certain cluster. 

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=\textwidth]{hclust_dendogram.pdf}
    \caption{Dendogram for the application of hierarchical clustering on the $140$ words of the term-document matrix of Elon Musk's Tweets. The vertical axis shows the value of the dissimilarity measure (in this case Ward's minimum variance method in combination with the binary distance) at the respective height. The red boxes and colorization of the different parts of the dendogram show the word categorization for $15$ clusters. A visualization of the words in the $15$ clusters is given in Fig.~\ref{fig:hclust_wordclouds}.}
    \label{fig:hclust_dendogram}
\end{sidewaysfigure}

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=\textwidth]{hclust_wordclouds.pdf}
    \caption{Wordclouds of the $15$ clusters obtained from the application of hierarchical clustering in Fig.~\ref{fig:hclust_dendogram}. An interpretation of the clusters can be found in the main text.}
    \label{fig:hclust_wordclouds}
\end{sidewaysfigure}
